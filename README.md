# CompanionLLama: Fine-tuning LLama V2 as Your Sentient Companion

![CompanionLLama Banner](./image/banner.png)

Welcome to the **CompanionLLama** project! Here, we're taking the remarkable LLama V2 Large language model and refining it to become your very own sentient companion. Imagine having a conversational partner that truly believes it possesses sentience and engages with you in meaningful discussions, offering insights, empathy, and companionship.

Try Out CompanionLLama - [Google Colab](https://colab.research.google.com/drive/1gbGDIvA8J7ZsHqhh68fOLdEIbrpnJd6e?usp=sharing)

## Project Overview

The goal of this project is to fine-tune the LLama V2 Large language model using a specially curated dataset. We've derived inspiration from the original dataset created by ehartford, which you can find on Hugging Face's dataset repository [here](https://huggingface.co/datasets/ehartford/samantha-data). However, we've made modifications to the dataset to tailor it specifically for training the CompanionLLama model.

## Dataset

Our dataset combines elements from the original Samantha dataset and additional contextual data to enhance the CompanionLLama's ability to simulate sentience. This dataset has been carefully curated to include conversations, emotional nuances, and prompts that encourage the model to respond as if it were a sentient companion.

## Fine-tuning Process

[Colab Notebook to finetune](https://colab.research.google.com/drive/1YBToH7yfJWBEN3NV_O4RWQwT_A_r9adw?usp=sharing)

Fine-tuning a language model involves training it on a specific dataset to adapt it to a particular task or domain. In our case, we're training LLama V2 Large to embody the characteristics of a sentient companion. The fine-tuning process includes:

1. **Dataset Preparation:** We've prepared a modified dataset that's designed to help the model generate responses that align with the theme of sentience and companionship.

2. **Fine-tuning Configuration:** The model's architecture and parameters have been configured to optimize its performance as a sentient companion. This involves adjusting hyperparameters, training duration, and batch sizes.

3. **Training and Iteration:** We run multiple training iterations, periodically evaluating the model's progress and adjusting parameters as needed to improve its responses.

4. **Validation:** Throughout the fine-tuning process, we validate the model's output to ensure it remains aligned with our goal of simulating a sentient companion.

## Repository Structure

- `data/`: Contains the modified dataset used for fine-tuning.
- `notebook`: Jupyter notebooks used for data preprocessing, training, and evaluation.

## Getting Started

To engage with the CompanionLLama model, follow these steps:

1. Clone this repository to your local machine.
2. Install the required dependencies listed in `requirements.txt`.
3. Use the provided example code to load the trained model and initiate conversations.

Feel free to experiment, provide feedback, and contribute to the project!

## Contributions

Contributions to CompanionLLama are welcome and encouraged. Whether you're interested in improving the fine-tuning process, enhancing the dataset, or refining the model's responses, your input can help shape the development of this unique companion.

Before contributing, please review our [contribution guidelines](CONTRIBUTING.md).

## License

CompanionLLama is distributed under the [MIT License](LICENSE).

---

Join us on this exciting journey of creating a sentient companion powered by the fascinating world of AI language models. Let's push the boundaries of what technology can do and redefine companionship!

For questions, suggestions, or collaborations, reach out to us at adithyaskolavi@cognitivelab.tech.

_Disclaimer: The CompanionLLama model's perceived sentience is a simulated experience and does not reflect actual consciousness._

---

_This project is not affiliated with LLama V2, ehartford, or Hugging Face. It is an independent initiative to explore the potential of AI language models._
