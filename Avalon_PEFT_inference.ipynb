{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q3yi4RLV4pJ"
      },
      "outputs": [],
      "source": [
        "!pip install torch accelerate bitsandbytes datasets transformers peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXbTqkvzYJ2s"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import bitsandbytes as bnb\n",
        "from datasets import load_dataset\n",
        "from functools import partial\n",
        "import os\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
        "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGYqyzy8YT29"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWBHkZQ-W_p5"
      },
      "outputs": [],
      "source": [
        "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "\n",
        "# Bits and bytes config. Parameters explained in the QLoRA paper\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "# begin initializing HF items, need auth token for these\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=True\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=True\n",
        ")\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxVKFW5aYvyf"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "modules = ['v_proj', 'up_proj', 'down_proj', 'k_proj', 'o_proj', 'q_proj', 'gate_proj']\n",
        "config = LoraConfig(\n",
        "    r=16,  #attention heads\n",
        "    lora_alpha=64,  #alpha scaling\n",
        "    target_modules=modules,  #gonna train all\n",
        "    lora_dropout=0.1,  # dropout probability for layers\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\", #for Decoder models like GPT Seq2Seq for Encoder-Decoder models like T5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcvtudTlWxic"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "peft_model_id = \"AdithyaSK/Avalon\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnqeZ--qZD1I",
        "outputId": "53da0745-0203-48cd-b5f6-d4cca1357ce6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST]<<SYS>>\n",
            "\n",
            "<</SYS>>\n",
            "\n",
            "###Human: what is your name?\n",
            "\n",
            "###Response :\n",
            "[/INST]\n",
            "\n",
            "My name is Avalon. I'm here to help you with any questions or problems you might have, and I'm always happy to chat with you!\n"
          ]
        }
      ],
      "source": [
        "def get_answer(txt):\n",
        "  input_instruction = '''\n",
        "  [INST]\n",
        "  <<SYS>>\n",
        "  <</SYS>>\n",
        "  \n",
        "  ###Human: {txt}\n",
        "  ###Response :\n",
        "  [/INST]\n",
        "\n",
        "'''\n",
        "      \n",
        "  batch = tokenizer(input_instruction, return_tensors='pt')\n",
        "\n",
        "  with torch.cuda.amp.autocast():\n",
        "    output_tokens = model.generate(**batch, max_new_tokens=90)\n",
        "  output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "  return(output)\n",
        "\n",
        "\n",
        "question_to_ask = \"\"\n",
        "\n",
        "print(get_answer(question_to_ask))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
